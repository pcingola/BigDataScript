
#-------------------------------------------------------------------------------
#
# BigDataScript configuration file
#
#																Pablo Cingolani
#-------------------------------------------------------------------------------

#---
# Default parameters
#---

# Default memory in bytes (negative number means unspecified)
# Strings finished by 'K', 'M', 'G', will be treated as Kilo, Mega, 
# Giga, etc. (2^10, 2^20, 2^30, etc.)
#mem = -1

# Default execution node (empty means unspecified)
#node = ""

# Add default queue name (empty means unspecified)
#queue = ""

# Default number of retries when a task fails (0 means no retry)
# Upon failire, a task is re-executed up to 'retry' times. 
# I.e. a task is considered failed only after failing 'retry + 1' times.
#retry = 0

# Default system type. 
# If unspecified, the default system is 'local' (run tasks on local computer)
#system = "local"

# Task timeout in seconds (default is one day)
#timeout = 86400

# Task's wall-timeout in seconds (default is one day). 
# Wall timeout includes all the time that the task is waiting to be executed. 
# I.e. the total amount of time we are willing to wait for a task to finish.
# For example if walltimeout is one day and a task is queued by the cluster 
# system for one day (and never executed), it will timeout, even if the task 
# was never run.
#walltimeout = 86400

# Shell to be used when running a task (default '/bin/sh -e')
#     WARNING: Make sure you use "-e" or some command line option that stops execution when an error if found.
#taskShell = /bin/sh -e

# Sys shell (or command execution shell)
#    WARNING: Make sure you use "-e" or some command line option that stops execution when an error if found.
#    WARNING: Make sure you use "-c" or some command line option that allows to provide a script
#sysShell = /bin/sh -e -c

# Maximum number of thread when executing 'runTask'
# Don't run too many threads at once when dispatching 
# tasks (e.g. running thousands of 'qsub' commands)
# The reason for this is that we can reach the maximum
# number of threads available in the operating system.
# If that happens, well get an exception
#maxThreads = 512

# After dispatching a task, wait for 'waitAfterTaskRun' milliseconds 
# (zero means do not wait). This is done in order to avoid / mitigate 
# problems that some clusters have when submitting many thousands of 
# tasks simultaneously. An example is when 'qsub' reports "Unable to 
# connect to socket" if there are many tasks being submitted 
# simultaneously.
#waitAfterTaskRun = 0

# This is a horrible hack used to make sure the shell script has
# been fully written to disk and we no have the file open for writing.
# Even if we closed the file, sometimes a "text file busy" error
# pops up. A better solution involves using something like 'lsof' 
# which doesn't seem to be available in Java
# Note: Time in miliseconds
#waitTextFileBusy = 1

# Maximum nuber of characters used when showing a task 'hint' (reports)
# Negative numbers means 'unlimited'
#taskMaxHintLen = 150

# Disable checkpoint creation when this option is set
#disableCheckpoint = false

# Disable removing files on exit
#disableRmOnExit = false

# Number of lines to use in file 'tail' (e.g. when showing tasks' output)
# A negative number means 'the whole file'
#tailLines = 10

# Filter out form task 'hint'
# Space spearated list of strings: If any line contains the string, it 
# is exluded from task's hint
# Default: Nothing
#filterOutTaskHint = 

# Always show task's code (i.e. sys commands) when the task info is 
# printed to STDOUT/STDERR. By default it is only shown when in debug mode.
#showTaskCode = false

# Temporary directory. It is used for several things, such as downloading 
# files from remote file systems
#tmpDir = /tmp

#---
# Cluster options
#---

# Regex used to extract PID from cluster command (e.g. qsub). 
# Default, use the whole line
#
# When bds dispatches a task to the cluster management system (e.g. running 
# 'qsub' command), it expects the cluster system to inform the jobID.
# Typically cluster systems show jobIDs in the first output line.
# This regex is used to match that jobID.
#
# Note: Some clusters add the domain name to the ID and 
#       then never use it again, some other clusters add 
#       a message (e.g. 'You job ...')
#pidRegex = ""
#pidRegex = "(.+).domain.com"
#pidRegex = (.+).guillimin.clumeq.ca
#pidRegex = "Your job (\\S+)"

# Regex used to extract PID from cluster's "check tasks" (e.g. qstat). 
# Default: Empty (use the first column)
#
# Every couple of minutes, bds checks that tasks are running in the cluster 
# by runing a 'qstat' command and parsing each output line as follows:
#   i) Matches regex using 'pidRegexCheckTaskRunning' (if any regex is set)
#   ii) Splits each out using '\\s+' and tries column number 'pidColumnCheckTaskRunning':
#       ii.a) The whole column
#       ii.b) Substring until first dot ('.')
#pidRegexCheckTaskRunning = ""

# Column number to use for "check tasks" (e.g. qstat)
# Note: Column numbers are one-based (i.e. first column is number '1', not number '0')
# Default: Use first column
# For an explanation, see 'pidRegexCheckTaskRunning'
#pidColumnCheckTaskRunning = 1

# These command line arguments are added to every cluster 'run' command (e.g. 'qsub')
# The string is split on spaces (regex: '\s+') and added to the cluster's run command.
#
# For instance the following configuration:
#
#       clusterRunAdditionalArgs = -A accountID -M user@gmail.com
#
#       Will be cause four additional arguments { '-A', 'accountID', '-M', 'user@gmail.com' } to 
#       be added immediately after 'qsub' (or similar) command used to run tasks on a cluster.
#clusterRunAdditionalArgs = 

# These command line arguments are added to every cluster 'kill' command (e.g. 'qdel')
# Same rules as 'clusterRunAdditionalArgs' apply
#clusterKillAdditionalArgs = 

# These command line arguments are added to every cluster 'stat' command (e.g. 'qstat')
# Same rules as 'clusterRunAdditionalArgs' apply
#clusterStatAdditionalArgs = 

# These command line arguments are added to every cluster 'post mortem info' command (e.g. 'qstat -f')
# Same rules as 'clusterRunAdditionalArgs' apply
#clusterPostMortemInfoAdditionalArgs = 

# Disable cluster post-mortem information: Some clusters do not provide any information
# after the process finished executing, so trying to find post-mortem info will always 
# result in an error (e.g. "Following jobs do not exist").
# Set this to 'true' to disable post mortem info
#clusterPostMortemDisabled=false

#---
# SGE parameters
#---

# Parallel environment in SGE (e.g. 'qsub -pe mpi 4')
# 
# Note on SGE's parallel environment ('-pe'):
#   Parallel environment defines how 'slots' (number of cpus requested) 
#   are allocated. StarCluster by default sets up a parallel environment, called “orte”, 
#   that has been configured for OpenMPI integration within SGE and has a number of slots 
#   equal to the total number of processors in the cluster.
#   See details 'qconf -sp orte':
#         pe_name            orte
#         slots              16
#         user_lists         NONE
#         xuser_lists        NONE
#         start_proc_args    /bin/true
#         stop_proc_args     /bin/true
#         allocation_rule    $round_robin
#         control_slaves     TRUE
#         job_is_first_task  FALSE
#         urgency_slots      min
#         accounting_summary FALSE
#         
#   Notice the allocation_rule = $round_robin.  This defines how to assign slots to a job. By 
#   default StarCluster configures round_robin allocation. This means that if a job requests 8 
#   slots for example, it will go to the first machine, grab a single slot if available, move to 
#   the next machine and grab a single slot if available, and so on wrapping around the cluster 
#   again if necessary to allocate 8 slots to the job.
#   You can also configure the parallel environment to try and localize slots as much as 
#   possible using the "fill_up" allocation rule and job_is_first_task of TRUE.
#   To configure: qconf -mp orte
#
#   References: 
#   	http://star.mit.edu/cluster/docs/0.93.3/guides/sge.html#openmpi-parallel-environment
#   	https://blogs.oracle.com/templedf/entry/configuring_a_new_parallel_environment
#
sge.pe = orte

# Parameter for requesting amount of memory in qsub (e.g. 'qsub -l mem 4G')
sge.mem = mem

# Parameter for timeout in qsub (e.g. 'qsub -l h_rt 24:00:00')
sge.timeout = h_rt

# Parameter for timeout in qsub (e.g. 'qsub -l s_rt 24:00:00')
sge.timeout2 = s_rt

#---
# Generic cluster
#
# Cluster 'generic' invokes each of these user defined scripts for manupulating tasks
# This allows the user to customize scripts for particular cluster environments not
# currently supproted by bds
#
# Note: You should either provide the script's full path or the scripts should 
#       be in your PATH
#
# Note: These scripts "comunicate" with bds by printing information on STDOUT. The 
#       information has to be printed in a very specific format. Failing to adhere 
#       to the format will cause bds to fail in unexpected ways.
#
# Note: You can use command path starting with '~' to indicate HOME dir or '.' to 
#       indicate path relative to config's file dir
#---

# The following script is called when a task is submitted to the cluster
#
# Script's output:
#     The script MUST print the cluster's jobID AS THE FIRST LINE. 
#     Make sure to flush STDOUT to avoid other lines to be printed out of order.
#
# Command line arguments:
#     1) Task's timeout in seconds. Negative number means 'unlimited' (i.e. let the cluster system decide)
#     2) Tasks required CPUs: number of cores within the same node.
#     3) Task's required memory in bytes. Negative means 'unspecified' (i.e. let the cluster system decide)
#     4) Cluster's queue name. Empty means "use cluster's default"
#     5) Cluster's STDOUT redirect file. This is where the cluster should redirect STDOUT.
#     6) Cluster's STDERR redirect file. This is where the cluster should redirect STDERR
#     7) Cluster command and arguments to be executed (typically is a "bds -exec ...").

clusterGenericRun = ~/.bds/clusterGeneric/run.pl

# The following command is executed in order to kill a task
#
# Script's output: 
#     None
#
# Command line arguments: 
#     jobId: This is the jobId returned as the first line in 'clusterGenericRun' 
#           script (i.e. the jobID provided by the cluster management system)

clusterGenericKill = ~/.bds/clusterGeneric/kill.pl

# The following command is executed in order to show the jobID of all jobs currently 
# scheduled in the cluster
#
# Script's output: 
#     This script is expected to print all jobs currently scheduled or 
#     running in the cluster (e.g. qstat). One per line. The FIRST column 
#     should be the jobID (columns are spce or tab separated). Other 
#     columns may exists (but are currently ignored).
#
# Command line arguments: 
#     None

clusterGenericStat = ~/.bds/clusterGeneric/stat.pl

# The following command is executed in order to get information of a recently 
# finished jobId. This information is typically used for debuging and it added to bds's output.
#
# Script's output: 
#     The output is not parsed, it is stored and later shown 
#     in bds's report. Is should contain information relevant 
#     to the job's execution (e.g. "qstat -f $jobId" or 
#     "checkjob -v $jobId")
#
# Command line arguments: 
#     jobId: This is the jobId returned as the first line in 'clusterGenericRun' 
#           script (i.e. the jobID provided by the cluster management system)

clusterGenericPostMortemInfo = ~/.bds/clusterGeneric/postMortemInfo.pl

#---
# SSH cluster nodes stored here
# 
# Format: user@host[:port]
#---

# Ssh cluster: Localhost (testing)
ssh.nodes = user@localhost

# Some nodes for 'ssh cluster'
#ssh.nodes = user@lab1-1company.com, user@lab1-2company.com, user@lab1-3company.com, user@lab1-4company.com, user@lab1-5company.com, user@lab1-6company.com, user@lab1-7company.com, user@lab1-8company.com, user@lab1-9company.com, user@lab1-10company.com, user@lab1-11company.com, user@lab1-12company.com, user@lab1-13company.com, user@lab1-14company.com, user@lab1-15company.com, user@lab1-16company.com \
# 			, user@lab2-1company.com, user@lab2-2company.com, user@lab2-3company.com, user@lab2-4company.com, user@lab2-5company.com, user@lab2-6company.com, user@lab2-7company.com, user@lab2-8company.com, user@lab2-9company.com, user@lab2-10company.com, user@lab2-11company.com, user@lab2-12company.com, user@lab2-13company.com, user@lab2-14company.com, user@lab2-15company.com, user@lab2-16company.com, user@lab2-17company.com, user@lab2-18company.com, user@lab2-19company.com, user@lab2-20company.com, user@lab2-21company.com, user@lab2-22company.com, user@lab2-23company.com, user@lab2-24company.com, user@lab2-25company.com, user@lab2-26company.com, user@lab2-27company.com, user@lab2-28company.com, user@lab2-29company.com, user@lab2-30company.com, user@lab2-31company.com, user@lab2-32company.com, user@lab2-33company.com, user@lab2-34company.com, user@lab2-35company.com, user@lab2-36company.com, user@lab2-37company.com, user@lab2-38company.com, user@lab2-39company.com, user@lab2-40company.com, user@lab2-41company.com, user@lab2-42company.com, user@lab2-43company.com, user@lab2-44company.com, user@lab2-45company.com, user@lab2-46company.com, user@lab2-47company.com, user@lab2-48company.com, user@lab2-49company.com \
# 			, user@lab3-1company.com, user@lab3-2company.com, user@lab3-3company.com, user@lab3-4company.com, user@lab3-5company.com, user@lab3-6company.com, user@lab3-7company.com, user@lab3-8company.com, user@lab3-9company.com, user@lab3-10company.com, user@lab3-11company.com, user@lab3-12company.com, user@lab3-13company.com, user@lab3-14company.com, user@lab3-15company.com, user@lab3-16company.com, user@lab3-17company.com, user@lab3-18company.com, user@lab3-19company.com, user@lab3-20company.com, user@lab3-21company.com, user@lab3-22company.com, user@lab3-23company.com, user@lab3-24company.com, user@lab3-25company.com, user@lab3-26company.com, user@lab3-27company.com, user@lab3-28company.com
#

# AWS server farm using ssh (nodes started using StarCluster)
#ssh.nodes = sgeadmin@node001, sgeadmin@node002, sgeadmin@node003, sgeadmin@node004, sgeadmin@node005, sgeadmin@node006

#---
# Mesos parameters
#
# Note: Mesos native library is expected in .bds/lib direcotry 
#       (with name 'libmesos.so'). So you may have to run something 
#       like the following commands:
#           cd ~/.bds/
#           mkdir lib
#           ln -s /usr/local/lib/libmesos-0.26.0.so libmesos.so
#---

# Mesos master IP address and port
#mesos.master = 127.0.0.1:5050

#---
# Amazon (AWS) parameters
#---

# Amazon region
# awsRegion = US_WEST_2
